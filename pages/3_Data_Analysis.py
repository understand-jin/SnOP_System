# pages/3_Data_Analysis.py
import streamlit as st
import pandas as pd
from datetime import datetime

st.set_page_config(page_title="S&OP - Data Analysis", layout="wide")
st.title("ğŸ“ˆ Data Analysis")

# =====================================================
# ğŸ”§ dfs key ì´ë¦„ (ì‹¤ì œ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ) (ì¬ê³  ë°ì´í„°)
# =====================================================
STOCK_DF_KEY = "2. ë°°ì¹˜ ì¬ê³ ìˆ˜ë¶ˆë¶€(ë°°ì¹˜).xls"
EXPIRY_DF_KEY = "3. ì°½ê³ ë³„ ì¬ê³ í˜„í™©(ìœ íš¨ê¸°í•œ)_1.19.xls"

BATCH_COL = "ë°°ì¹˜"
EXPIRY_COL = "ìœ íš¨ ê¸°í•œ"

# âœ… ì¬ê³ ìˆ˜ëŸ‰ì€ ì´ ì»¬ëŸ¼ì„ ì‚¬ìš©!
STOCK_QTY_SOURCE_COL = "Stock Quantity on Period End"
QTY_COL = "ì¬ê³ ìˆ˜ëŸ‰"     # ìµœì¢… í†µí•© DFì—ì„œ ì“¸ í‘œì¤€ ì»¬ëŸ¼ëª…

# ê¸ˆì•¡ ì»¬ëŸ¼ì€ ì•„ì§ í™•ì • ì—†ìœ¼ë‹ˆ(ì—†ì–´ë„ ë™ì‘í•˜ê²Œ) optional ì²˜ë¦¬
VAL_COL = "ì¬ê³ ê¸ˆì•¡"

# =====================================================
# ì¬ê³ ìœ í‹¸
# =====================================================
def to_datetime_safe(s):
    return pd.to_datetime(s, errors="coerce")

def build_stock_df(df_stock: pd.DataFrame, df_expiry: pd.DataFrame):
    # í•„ìš”í•œ ì»¬ëŸ¼ ì²´í¬
    if BATCH_COL not in df_stock.columns:
        raise ValueError(f"ì¬ê³  ë°ì´í„°ì— '{BATCH_COL}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    if STOCK_QTY_SOURCE_COL not in df_stock.columns:
        raise ValueError(
            f"ì¬ê³  ë°ì´í„°ì— ì¬ê³ ìˆ˜ëŸ‰ ì›ì²œ ì»¬ëŸ¼ '{STOCK_QTY_SOURCE_COL}' ì´(ê°€) ì—†ìŠµë‹ˆë‹¤."
        )

    if BATCH_COL not in df_expiry.columns or EXPIRY_COL not in df_expiry.columns:
        raise ValueError(f"ìœ íš¨ê¸°í•œ ë°ì´í„°ì— '{BATCH_COL}', '{EXPIRY_COL}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ìœ íš¨ê¸°í•œ DF ì •ë¦¬
    df_expiry2 = df_expiry[[BATCH_COL, EXPIRY_COL]].copy()
    df_expiry2[EXPIRY_COL] = to_datetime_safe(df_expiry2[EXPIRY_COL])

    # âœ… ë°°ì¹˜ ê¸°ì¤€ ë³‘í•©
    merged = df_stock.merge(df_expiry2, on=BATCH_COL, how="left")

    # âœ… ìµœì¢… í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ ì¬ê³ ìˆ˜ëŸ‰ ë§Œë“¤ê¸°
    merged[QTY_COL] = pd.to_numeric(merged[STOCK_QTY_SOURCE_COL], errors="coerce").fillna(0)

    # ê¸ˆì•¡ ì»¬ëŸ¼ì€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ (ë‚˜ì¤‘ì— ì›ê°€/ë‹¨ê°€ ë¶™ì´ë©´ í™•ì¥ ê°€ëŠ¥)
    if VAL_COL not in merged.columns:
        merged[VAL_COL] = 0
    merged[VAL_COL] = pd.to_numeric(merged[VAL_COL], errors="coerce").fillna(0)

    # í’ˆì§ˆ
    quality = {
        "rows": len(merged),
        "mapped_expiry_rate": float(merged[EXPIRY_COL].notna().mean()),
        "missing_expiry_rows": int(merged[EXPIRY_COL].isna().sum())
    }
    return merged, quality

def add_expiry_bucket(df: pd.DataFrame):
    df = df.copy()
    today = pd.Timestamp(datetime.now().date())

    # ë‚ ì§œ ë³€í™˜(ë¬¸ìì—´ ì„ì—¬ ìˆì–´ë„ ì•ˆì „)
    df[EXPIRY_COL] = to_datetime_safe(df[EXPIRY_COL])

    # ë‚¨ì€ ì¼ìˆ˜
    df["days_to_expiry"] = (df[EXPIRY_COL] - today).dt.days

    # âœ… ìš”ì²­í•œ ë²„í‚· êµ¬ê°„
    # ë²„ì»· : ì—°ì†ì ì¸ ê°’ì„ ì˜ë¯¸ ìˆëŠ” êµ¬ê°„(ë²”ì£¼)ë¡œ ë¬¶ëŠ” ê²ƒ
    def bucketize(days):
        if pd.isna(days):
            return "ìœ íš¨ê¸°í•œ ì—†ìŒ"
        if days <= 0:
            return "íê¸°í™•ì •(ìœ íš¨ê¸°í•œ ì§€ë‚¨)"
        if days <= 90:
            return "3ê°œì›” ë¯¸ë§Œ"
        if days <= 180:
            return "6ê°œì›” ë¯¸ë§Œ"
        if days <= 270:
            return "9ê°œì›” ë¯¸ë§Œ"
        if days <= 365:
            return "12ê°œì›” ë¯¸ë§Œ"
        if days <= 540:
            return "18ê°œì›” ë¯¸ë§Œ"
        if days <= 730:
            return "24ê°œì›” ë¯¸ë§Œ"
        return "24ê°œì›” ì´ìƒ"

    df["expiry_bucket"] = df["days_to_expiry"].apply(bucketize)

    # ë³´ê¸° ì¢‹ì€ ìˆœì„œ
    bucket_order = [
        "íê¸°í™•ì •(ìœ íš¨ê¸°í•œ ì§€ë‚¨)",
        "3ê°œì›” ë¯¸ë§Œ",
        "6ê°œì›” ë¯¸ë§Œ",
        "9ê°œì›” ë¯¸ë§Œ",
        "12ê°œì›” ë¯¸ë§Œ",
        "18ê°œì›” ë¯¸ë§Œ",
        "24ê°œì›” ë¯¸ë§Œ",
        "24ê°œì›” ì´ìƒ",
        "ìœ íš¨ê¸°í•œ ì—†ìŒ",
    ]
    df["expiry_bucket"] = pd.Categorical(df["expiry_bucket"], categories=bucket_order, ordered=True)

    # âœ… ìš”ì•½
    summary = (
        df.groupby("expiry_bucket", as_index=False)[[QTY_COL, VAL_COL]]
          .sum()
          .sort_values("expiry_bucket")
    )

    # KPI
    expired_mask = df["expiry_bucket"] == "íê¸°í™•ì •(ìœ íš¨ê¸°í•œ ì§€ë‚¨)"
    kpi = {
        "today": str(today.date()),
        "total_qty": float(df[QTY_COL].sum()),
        "expired_qty": float(df.loc[expired_mask, QTY_COL].sum()),
        "expired_ratio": float(df.loc[expired_mask, QTY_COL].sum() / df[QTY_COL].sum()) if df[QTY_COL].sum() else 0.0,
    }

    return df, summary, kpi

# =====================================================
# dfs ë¡œë“œ
# =====================================================
dfs = st.session_state.get("dfs")
if dfs is None:
    st.warning("ë¨¼ì € ì—…ë¡œë“œ í˜ì´ì§€ì—ì„œ Raw ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

if STOCK_DF_KEY not in dfs or EXPIRY_DF_KEY not in dfs:
    st.error("í•„ìš”í•œ ì¬ê³ /ìœ íš¨ê¸°í•œ íŒŒì¼ì´ dfsì— ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ íŒŒì¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

df_stock = dfs[STOCK_DF_KEY]
df_expiry = dfs[EXPIRY_DF_KEY]

# =====================================================
# ğŸš€ ìë™ ë³‘í•© & ìƒì„±
# =====================================================
st.subheader("âœ… ì¬ê³  ìœ íš¨ê¸°í•œ ë°ì´í„° ìë™ ìƒì„±")

with st.spinner("ë°°ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨ê¸°í•œì„ ë³‘í•©í•˜ê³ , ìœ íš¨ê¸°í•œ êµ¬ê°„ì„ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤..."):
    merged_df, quality = build_stock_df(df_stock, df_expiry)
    stock_df2, expiry_summary, kpi = add_expiry_bucket(merged_df)

# =====================================================
# ğŸ“¦ data_registryì— ë“±ë¡ (ì—¬ëŸ¬ ë°ì´í„° ê´€ë¦¬ìš©)
# =====================================================
if "data_registry" not in st.session_state:
    st.session_state["data_registry"] = {"datasets": {}, "selected_id": None}

dataset_id = f"stock_expiry_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
st.session_state["data_registry"]["datasets"][dataset_id] = {
    "domain": "stock",
    "title": "ì¬ê³  ìœ íš¨ê¸°í•œ(íê¸°~24ê°œì›”+) ë¶„ë¥˜",
    "df": stock_df2,
    "summary": expiry_summary,
    "kpi": kpi,
    "quality": quality,
    "source": [STOCK_DF_KEY, EXPIRY_DF_KEY],
    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
}
st.session_state["data_registry"]["selected_id"] = dataset_id

# =====================================================
# ğŸ“Š í™”ë©´ ì¶œë ¥
# =====================================================
c1, c2, c3, c4 = st.columns(4)
c1.metric("ìœ íš¨ê¸°í•œ ë§¤í•‘ë¥ ", f"{quality['mapped_expiry_rate']*100:.1f}%")
c2.metric("ì´ í–‰ ìˆ˜", f"{quality['rows']:,}")
c3.metric("íê¸°í™•ì • ìˆ˜ëŸ‰", f"{kpi['expired_qty']:,.0f}")
c4.metric("íê¸°í™•ì • ë¹„ì¤‘", f"{kpi['expired_ratio']*100:.1f}%")

st.write("### âœ… ìœ íš¨ê¸°í•œ êµ¬ê°„ ìš”ì•½")
st.dataframe(expiry_summary, use_container_width=True)

st.write("### âœ… ìƒˆ ì¬ê³  ìœ íš¨ê¸°í•œ ë°ì´í„°(ë¯¸ë¦¬ë³´ê¸°)")
# í•µì‹¬ ì»¬ëŸ¼ ìœ„ì£¼ë¡œ ë¨¼ì € ë³´ì—¬ì£¼ê¸°
preview_cols = [c for c in [BATCH_COL, EXPIRY_COL, "days_to_expiry", "expiry_bucket", STOCK_QTY_SOURCE_COL, QTY_COL] if c in stock_df2.columns]
st.dataframe(stock_df2[preview_cols].head(80), use_container_width=True)

# =====================================================
# â¬‡ï¸ ë‹¤ìš´ë¡œë“œ
# =====================================================
st.divider()
csv_bytes = stock_df2.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "ğŸ“¥ ì¬ê³  ìœ íš¨ê¸°í•œ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
    data=csv_bytes,
    file_name=f"{dataset_id}.csv",
    mime="text/csv"
)
